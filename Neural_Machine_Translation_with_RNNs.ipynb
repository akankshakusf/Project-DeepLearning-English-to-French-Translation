{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akankshakusf/Project-DeepLearning-English-to-French-Translation/blob/master/Neural_Machine_Translation_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "45CHLgAvuG-q"
      },
      "outputs": [],
      "source": [
        "#import ML packages\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from sklearn.metrics import confusion_matrix,roc_curve\n",
        "import pathlib\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "\n",
        "#import DL package\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer,Dense, Flatten, InputLayer, BatchNormalization, Bidirectional, Dropout, Input, Embedding, TextVectorization\n",
        "from tensorflow.keras.layers import SimpleRNN, Conv1D, LSTM, GRU\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy, TopKCategoricalAccuracy, TopKCategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "mIsUJ6KO0ir8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Download"
      ],
      "metadata": {
        "id": "hNKV6ywY0tHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ],
      "metadata": {
        "id": "odODFPY10r0O",
        "outputId": "b24ce63d-f7a6-4ac9-f5fe-291cc24675ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-13 11:33:02--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.57M  4.19MB/s    in 1.8s    \n",
            "\n",
            "2025-04-13 11:33:05 (4.19 MB/s) - ‘fra-eng.zip’ saved [7943074/7943074]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "3OB777Yd0rw-",
        "outputId": "04d35d3a-5ae9-4a1a-f2b6-814912642032",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kaggle Dataset"
      ],
      "metadata": {
        "id": "Tm-6-2710xV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d dhruvildave/en-fr-translation-dataset"
      ],
      "metadata": {
        "id": "83td25ws0ZCE",
        "outputId": "075a1983-2649-442b-9707-a1f075084697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/dhruvildave/en-fr-translation-dataset\n",
            "License(s): ODbL-1.0\n",
            "en-fr-translation-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/en-fr-translation-dataset.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "AxPKfUkE5TQ0",
        "outputId": "b17bcf05-6013-4a3e-9145-80a9e025fe11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/en-fr-translation-dataset.zip\n",
            "  inflating: /content/dataset/en-fr.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.experimental.CsvDataset(\n",
        "  \"/content/dataset/en-fr.csv\",\n",
        "  [\n",
        "    tf.string,\n",
        "    tf.string\n",
        "  ],\n",
        ")"
      ],
      "metadata": {
        "id": "qYAbSuXo0w7j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "qj-M9VE86GlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset=tf.data.TextLineDataset(\"/content/dataset/fra.txt\")"
      ],
      "metadata": {
        "id": "2THpKCsm0w4s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#review dataset\n",
        "for i in text_dataset.take(3):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "ulVcvBdT0mPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619215b5-b7ca-4f4c-dbcd-fb33fe2c2082"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #lets skip a max number of records and check what max length we find\n",
        "# for i in text_dataset.skip(190000):\n",
        "#   print(len(tf.strings.split(i,\" \")))"
      ],
      "metadata": {
        "id": "vKU9oT6S0mM1",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since i saw that the max len of the sentence is 107. I am going to go ahead with a sequence length of 64 as we also have french letters"
      ],
      "metadata": {
        "id": "Xd0sx_-MuuLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE= 20000\n",
        "SEQUENCE_LENGTH=64\n",
        "EMBEDDING_DIM = 300"
      ],
      "metadata": {
        "id": "--NXDb7RxLV-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create vectorizer layer to create vectors\n",
        "- reference :https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization"
      ],
      "metadata": {
        "id": "wtM_PoJpxYyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorize_layer = TextVectorization (\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length = SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "Gj-ZQvGRxem2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorize_layer = TextVectorization (\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length = SEQUENCE_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "1zXDZOlExekR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Look at this sample data from dataset and get rid of tabs--->  \\t\n",
        "'Go.\\tVa !\\tCC-BY 2.0 (France)"
      ],
      "metadata": {
        "id": "VG1V7ioQ0UxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def selector (input_text):\n",
        "  split_text=tf.strings.split(input_text, '\\t')\n",
        "  #after splitting collect english and french separately\n",
        "  return split_text[0:1], split_text[1:2]"
      ],
      "metadata": {
        "id": "3X50IU-w0hfV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#map text_dataset to selector above function\n",
        "split_dataset = text_dataset.map(selector)"
      ],
      "metadata": {
        "id": "dr5LdXgK1IZl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# review the data\n",
        "for i in split_dataset.take(2):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMv6N5Oo1VEg",
        "outputId": "8a95bc13-57bb-489e-8d43-69b7b48fb73c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va !'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche.'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Notice how nicely english and french text have been separated now"
      ],
      "metadata": {
        "id": "POImeH-61gM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets attach this Vectorizer to split_dataset to get the vocabulary list\n",
        "english_training_dataset = split_dataset.map(lambda x,y:x) ##input is x, y and output is x\n",
        "english_vectorize_layer.adapt(english_training_dataset) ##adapth the vectorizer layer to training data"
      ],
      "metadata": {
        "id": "Hpm8-oPOyizU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now lets attach this Vectorizer to split_dataset to get the vocabulary list\n",
        "french_training_dataset = split_dataset.map(lambda x,y:x) ##input is x, y and output is x\n",
        "english_vectorize_layer.adapt(french_training_dataset) ##adapth the vectorizer layer to training data"
      ],
      "metadata": {
        "id": "EhjbC0FC15im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HA6Msvqu15gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2f-8iua15dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yBeRD80015aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "tM0uHoQh0nh9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkgQ1USc0n9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "o7ZU4sMi087i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "clQxKilx0-EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "he4jrcUd1B1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1vz9nDS1By2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "RFX8_wHp1COa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fyMmQkgn1DOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}